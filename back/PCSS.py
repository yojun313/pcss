from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_ollama import OllamaLLM
import platform
import aiofiles
from bs4 import BeautifulSoup
from user_agent import generate_navigator
import requests
import random
import traceback
import urllib3
import warnings
import pandas as pd
import re
import os
import aiohttp
import copy
from datetime import datetime
import asyncio
import json
import sys

TIMEOUT = 10
TRYNUM = 10

com = 'cluster'

if sys.platform == 'darwin':
    com = 'z8'
    
if com == 'z8':
    LLM_SERVER = '121.152.225.232'
    PORT = "3333"
elif com == 'cluster':
    LLM_SERVER = '141.223.16.196'
    PORT = "8089"

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

class PCSSEARCH:
    def __init__(self, option, threshold, startyear, endyear, countOption=True):

        self.option         = option
        self.threshold      = threshold
        self.startyear      = int(startyear)
        self.endyear        = int(endyear)       
        self.countOption    = countOption             

        self.proxy_option   = False
        self.proxy_path     = "C:/Users/magel/Documents/ì•„ì´í”¼ìƒµ(ìœ ë™í”„ë¡ì‹œ).txt"
        self.speed          = 3
        self.current_year   = 2025
        
        self.json_filename  = os.path.join(os.path.dirname(__file__), 'data', "llm_name.json")
        self.name_dict      = self.load_name_dict()

        self.llm_api_option = True
        self.api_url = f"http://{LLM_SERVER}:{PORT}/api/process"

        self.llm_model = 'llama3.3:70b-instruct-q8_0'
        if self.llm_api_option == False:
            self.llm = OllamaLLM(model=self.llm_model)
        self.checkedNameList = []
        self.titleList = []

        self.conf_df = pd.read_csv(os.path.join(os.path.dirname(__file__), 'data', 'conf.csv'))
        self.conf_param_list = self.conf_df['param'].tolist()
        self.conf_param_dict = self.conf_df.set_index('conference')['param'].to_dict()
        self.CrawlData = []
        self.FinalData = {}

        self.log_file_path = os.path.join(os.path.dirname(__file__), 'log', f"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.txt")  # ë¡œê·¸ íŒŒì¼ ì´ë¦„
        self.db_path = os.path.join(os.path.dirname(__file__), 'db')
        
        if self.proxy_option == True:
            self.init_proxy()

    def init_proxy(self):
        with open(self.proxy_path, "r", encoding="utf-8") as f:
            self.proxy_list = [line.strip() for line in f]  # strip()ì„ ì‚¬ìš©í•˜ì—¬ ê°œí–‰ ë¬¸ì ì œê±° (í•„ìš”í•œ ê²½ìš°)
    
    def async_proxy(self):
        proxy_server = random.choice(self.proxy_list)
        if self.proxy_option == True:
            return 'http://' + str(proxy_server)
        else:
            return None
    
    # í•œ Conferenceì— ëŒ€í•œ ì—°ë„ë³„ url í¬ë¡¤ë§ í•¨ìˆ˜
    async def conf_crawl(self, conf, session, conf_name):
        try:
            self.printStatus(f"{conf_name} Loading...", url=f"https://dblp.org/db/conf/{conf}/index.html")
            filtered_urls = []
            urls = []
            
            folder_path = os.path.join(os.path.dirname(__file__), 'data', 'urls')
            file_path = os.path.join(folder_path, f"{conf_name}.txt")
            
            if os.path.exists(file_path) and self.endyear != self.current_year:
                # ì´ë¯¸ íŒŒì¼ì´ ìˆë‹¤ë©´, í•´ë‹¹ ë‚´ìš© ì‚¬ìš©
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    async for line in f:
                        line = line.strip()
                        if not line:
                            continue
                        urls.append(line)
            else:
                response = await self.asyncRequester(f"https://dblp.org/db/conf/{conf}/index.html", session=session)
                if isinstance(response, tuple) == True:
                    return response
                self.printStatus(f"{conf_name} URL Crawling...", url=f"https://dblp.org/db/conf/{conf}/index.html")

                soup = BeautifulSoup(response, "lxml")

                links = soup.find_all('a', class_='toc-link')
                urls = [link['href'] for link in links if link['href']]
                
                with open(file_path, 'w', encoding='utf-8') as f:
                    for url in urls:
                        f.write(url + '\n')

            for url in urls:
                match = re.search(r'\d{4}', url)  # 4ìë¦¬ ìˆ«ì ì°¾ê¸°
                if match:
                    year_str = match.group()
                    if year_str.isdigit():  
                        year = int(year_str)
                        if self.startyear <= year <= self.endyear:
                            filtered_urls.append((url, year))
                            
            return filtered_urls
        except:
            self.write_log(traceback.format_exc())
            return []

    # í•œ ê°œì˜ Paperì— ëŒ€í•œ í¬ë¡¤ë§ í•¨ìˆ˜
    async def paper_crawl(self, conf, url, year, session):
        try:
            self.printStatus(f"{year} {conf} Loading...", url=url)
            param = self.conf_param_dict[conf]
            
            edited_url = re.sub(r'[^\w\-_]', '_', url) + ".html"
            edited_url = edited_url.replace('https___', '').replace('_html', '')
            
            record_path = os.path.join(self.db_path, param, edited_url)
            
            # ë¹„ë™ê¸° íŒŒì¼ ì½ê¸°: íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ aiofilesë¡œ ì½ìŒ
            if os.path.exists(record_path):
                async with aiofiles.open(record_path, "r", encoding="utf-8") as file:
                    response = await file.read()
            else:
                response = await self.asyncRequester(url, session=session)
                if year != 2025:
                    async with aiofiles.open(record_path, "w", encoding="utf-8") as file:
                        await file.write(response)
                
            
            if isinstance(response, tuple):
                return response         

            # CPU ë°”ìš´ë“œ íŒŒì‹± ì‘ì—…ì€ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
            soup = await asyncio.to_thread(BeautifulSoup, response, "lxml")
            
            # li.entry.inproceedings íƒœê·¸ë¥¼ í•œ ë²ˆì— selectë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
            papers = soup.select('li.entry.inproceedings')

            self.printStatus(f"{year} {conf} Crawling...", url=url)
            
            # titleListë¥¼ ì§‘í•©ìœ¼ë¡œë„ ê´€ë¦¬(ì´ˆê¸°í™”)
            if not hasattr(self, "_titleSet"):
                self._titleSet = set(self.titleList)
            
            for paper in papers:
                try:
                    # ì œëª© ì¶”ì¶œ
                    title_tag = paper.select_one('span.title')
                    title = title_tag.get_text(strip=True) if title_tag else 'No title found'

                    # ì¤‘ë³µ ì²´í¬
                    if title in self._titleSet:
                        continue
                    self._titleSet.add(title)
                    self.titleList.append(title)

                    # ì €ì ì¶”ì¶œ
                    authors_origin = []
                    authors_url = []

                    # ì €ì ì •ë³´ë¥¼ í•œ ë²ˆì— select
                    author_tags = paper.select('span[itemprop="author"] > a[href]')
                    if not author_tags:
                        # ì €ìê°€ í•˜ë‚˜ë„ ì—†ê±°ë‚˜ a[href]ê°€ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš°
                        continue

                    for a in author_tags:
                        author_name_tag = a.select_one('span[itemprop="name"]')
                        if author_name_tag:
                            authors_origin.append(author_name_tag.get_text(strip=True))
                        authors_url.append(a['href'])

                    # authors_origin ìˆê³ , authors_urlì´ í•˜ë‚˜ë„ ì—†ëŠ” ê²½ìš°ëŠ” skip
                    if authors_origin and not authors_url:
                        continue

                    if not authors_origin:
                        continue

                    # ì¡°ê±´ë³„ í•„í„°ë§/ì €ì¥
                    # ----------------------------------------------------
                    authors = authors_origin
                    def store_if_korean(idx_list):
                        """idx_listì— í•´ë‹¹í•˜ëŠ” ì €ìê°€ í•œêµ­ì¸ì´ë©´ ì €ì¥"""
                        target_authors = []
                        for idx in idx_list:
                            if idx < len(authors) and self.koreanChecker(authors[idx]):
                                # ì´ë¯¸ name_dictì— ê°’ì´ ìˆì„ ê²ƒì´ë¯€ë¡œ ê°€ì ¸ì˜¤ê¸°
                                target_authors.append(
                                    authors[idx] + f'({self.name_dict[authors[idx]]})'
                                )
                        return target_authors

                    if self.option == 1:
                        # 1ì €ì
                        if self.koreanChecker(authors[0]):
                            self.CrawlData.append({
                                'title': title,
                                'author_name': authors,
                                'author_url': authors_url,
                                'target_author': [authors[0] + f'({self.name_dict[authors[0]]})'],
                                'conference': conf,
                                'year': year,
                                'source': url
                            })
                    elif self.option == 2:
                        # 1ì €ì ë˜ëŠ” 2ì €ì
                        target = store_if_korean([0, 1])  # 0,1ì¸ë±ìŠ¤
                        if target:
                            self.CrawlData.append({
                                'title': title,
                                'author_name': authors,
                                'author_url': authors_url,
                                'target_author': target,
                                'conference': conf,
                                'year': year,
                                'source': url
                            })
                    elif self.option == 3:
                        # ë§ˆì§€ë§‰ ì €ì
                        if self.koreanChecker(authors[-1]):
                            self.CrawlData.append({
                                'title': title, 
                                'author_name': authors,
                                'author_url': authors_url,
                                'target_author': [authors[-1] + f'({self.name_dict[authors[-1]]})'],
                                'conference': conf,
                                'year': year,
                                'source': url
                            })
                    elif self.option == 4:
                        # 1ì €ì ë˜ëŠ” ë§ˆì§€ë§‰ ì €ì
                        target = []
                        if self.koreanChecker(authors[0]):
                            target.append(authors[0] + f'({self.name_dict[authors[0]]})')
                        if len(authors) > 1 and self.koreanChecker(authors[-1]):
                            target.append(authors[-1] + f'({self.name_dict[authors[-1]]})')
                        if target:
                            self.CrawlData.append({
                                'title': title,
                                'author_name': authors,
                                'author_url': authors_url,
                                'target_author': target,
                                'conference': conf,
                                'year': year,
                                'source': url
                            })
                    else:
                        # ì €ì ì¤‘ í•œ ëª… ì´ìƒì´ í•œêµ­ì¸
                        target = []
                        for auth in authors:
                            if self.koreanChecker(auth):
                                target.append(auth + f'({self.name_dict[auth]})')
                        if target:
                            self.CrawlData.append({
                                'title': title,
                                'author_name': authors,
                                'author_url': authors_url,
                                'target_author': target,
                                'conference': conf,
                                'year': year,
                                'source': url
                            })
                    # ----------------------------------------------------
                except:
                    self.write_log(traceback.format_exc())

        except:
            self.write_log(traceback.format_exc())

    # í•œ Conferenceì— ëŒ€í•œ ë³‘ë ¬ Paper í¬ë¡¤ë§ í•¨ìˆ˜
    async def MultiPaperCollector(self, conf_urls, conf_name, session):
        try:
            tasks = []
            for conf_url in conf_urls:
                try:
                    url = conf_url[0]
                    year = int(conf_url[1])
                    tasks.append(self.paper_crawl(conf_name, url, year, session))
                except:
                    self.write_log(f"{conf_url[1]}")
            results = await asyncio.gather(*tasks)
        except:
            self.write_log(traceback.format_exc())


    async def MultiConfCollector(self, conf_list):
        try:
            # í•˜ë‚˜ì˜ ì„¸ì…˜ì„ ì¬ì‚¬ìš©í•˜ë©° ê´€ë¦¬ (async with ì‚¬ìš©)
            async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit=self.speed)) as session:
                # ê° ì»¨í¼ëŸ°ìŠ¤ì— ëŒ€í•´ ë™ì‹œ í¬ë¡¤ë§ ìˆ˜í–‰
                async def process_conference(conf):
                    conf_name = conf
                    conf_param = self.conf_param_dict[conf_name]
                    conf_urls = await self.conf_crawl(conf_param, session, conf_name)
                    await self.MultiPaperCollector(conf_urls, conf_name, session)

                # ì»¨í¼ëŸ°ìŠ¤ í¬ë¡¤ë§ ì‘ì—…ë“¤ì„ ë³‘ë ¬ ì‹¤í–‰
                tasks = [process_conference(conf) for conf in conf_list]
                await asyncio.gather(*tasks, return_exceptions=True)

                # ì²« ë²ˆì§¸ ë‹¨ê³„ ì™„ë£Œ í›„, ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
                self.resultData = []

                # ì €ì í†µê³„ ì²˜ë¦¬ ë¹„ë™ê¸° í•¨ìˆ˜
                async def authorCounter(data):
                    data_copy = copy.deepcopy(data)
                    new_authors = []
                    multi_name_option = False  # í•„ìš”ì— ë”°ë¼ Trueë¡œ ë³€ê²½ ê°€ëŠ¥
                    
                    if not multi_name_option:
                        for index, author in enumerate(data_copy["author_name"]):
                            if not self.koreanChecker(author):
                                new_authors.append(author)
                                continue
                            stats = await self.authorNumChecker(author, data['author_url'][index], session)
                            new_authors.append(author + stats)
                    else:
                        llm_result = self.multi_name_llm(data_copy['author_name'])
                        for index, (author, result) in enumerate(llm_result.items()):
                            if not result:
                                new_authors.append(author)
                                continue
                            stats = await self.authorNumChecker(author, data['author_url'][index], session)
                    data_copy["author_name"] = new_authors
                    self.resultData.append(data_copy)

                if self.countOption:
                    # ê° ë°ì´í„°ì— ëŒ€í•´ ì €ì ì²˜ë¦¬ ì‘ì—…ë“¤ì„ ë³‘ë ¬ ì‹¤í–‰
                    tasks = [authorCounter(data) for data in self.CrawlData]
                    await asyncio.gather(*tasks, return_exceptions=True)
                else:
                    self.resultData = self.CrawlData

            # ì„¸ì…˜ì´ ì¢…ë£Œëœ í›„ì— ìµœì¢… ë°ì´í„°ë¥¼ ì •ë ¬ ë° JSON íŒŒì¼ë¡œ ì €ì¥
            self.FinalData = sorted(self.resultData, key=lambda x: (x["conference"], -x["year"]))
            self.FinalData = {index: element for index, element in enumerate(self.FinalData)}

            json_path = os.path.join(os.path.dirname(__file__), 'res', f"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.json")
            with open(json_path, 'w', encoding='utf-8') as json_file:
                json.dump(self.FinalData, json_file, ensure_ascii=False, indent=4)

            self.clear_console()
            print(f" PATH={json_path}")
            self.result_json_path = json_path
            return self.result_json_path

        except Exception as e:
            print(" PATH=ERROR", e)
            self.write_log(traceback.format_exc())

    # ë©”ì¸ í•¨ìˆ˜
    def main(self, conf_list):
        try:
            result_json_path = asyncio.run(self.MultiConfCollector(conf_list))

            return result_json_path
        except:
            self.write_log(traceback.format_exc())


    def koreanChecker(self, name, multi=False):
        if multi == False:
            self.printStatus(msg="LLM Checking Korean... ", url=name)
            if float(self.single_name_llm(name)) > self.threshold:
                if name not in self.checkedNameList:
                    self.checkedNameList.append(name)
                return True

            return False
        else:
            self.printStatus(msg="LLM Checking Korean... ", url=', '.join(name))
            result = self.multi_name_llm(name)
            
            result_dict = {}
            for key, value in result.items():
                if float(value) > self.threshold:
                    if name not in self.checkedNameList:
                        self.checkedNameList.append(name)
                    result_dict[key] = True
                else:
                    result_dict[key] = False
                    
            return result_dict
        # if self.possible == True:
        #     if name.split()[-1] in self.last_name_list and name.split()[0] in self.first_name_list and float(self.single_name_llm(name)) > self.possible_stat:
            
        # else:
        #     if name.split()[-1] in self.last_name_list and name.split()[0] in self.first_name_list and float(self.single_name_llm(name)) > 0.5:
            
        # return False


    def single_name_llm(self, name):
        
        try:
            return self.name_dict[name]
        except KeyError:
            pass
        
        if self.llm_api_option == False:
            template = "Express the likelihood of this {name} being Korean using only a number between 0~1. You need to say number only"

            prompt = PromptTemplate.from_template(template=template)
            chain = prompt | self.llm | StrOutputParser()

            result = chain.invoke({"name", name})

        else:
            result = self.llm_api_answer(
                query = f"Express the likelihood of this {name} being Korean using only a number between 0~1. You need to say number only",
                model = self.llm_model
            )

        # ğŸ”¹ ìˆ«ìë§Œ ì¶”ì¶œ (ì§€ìˆ˜ í‘œê¸°ë²• ë°©ì§€)
        match = re.findall(r"\d+\.\d+|\d+", result)
        if not match:
            return "0.0"  # ì˜ˆì™¸ ì²˜ë¦¬: ê²°ê³¼ê°€ ì—†ì„ ê²½ìš° ê¸°ë³¸ê°’

        value = float(match[0])  # ğŸ”¹ ë¬¸ìì—´ì„ floatìœ¼ë¡œ ë³€í™˜

        # ğŸ”¹ ìˆ«ì ë²”ìœ„ ê³ ì • (0.0 ~ 1.0)
        value = max(0.0, min(1.0, value))

        # ğŸ”¹ ì†Œìˆ˜ì  1ìë¦¬ê¹Œì§€ í¬ë§·íŒ…
        formatted_value = "{:.1f}".format(value)

        self.name_dict[name] = formatted_value
        self.save_name_dict()

        return formatted_value  # ğŸ”¹ ê²°ê³¼ ë°˜í™˜ (0.0 ~ 1.0)


    def multi_name_llm(self, names):
        result_dict = {}
        remaining_names = [name for name in names if name not in self.name_dict]

        # ê¸°ì¡´ì— ì €ì¥ëœ ê°’ ì¶”ê°€
        for name in names:
            if name in self.name_dict:
                result_dict[name] = self.name_dict[name]

        if remaining_names:  # ìƒˆë¡œìš´ ê°’ì„ ì¡°íšŒí•  í•„ìš”ê°€ ìˆëŠ” ê²½ìš°ë§Œ ì‹¤í–‰
            query = f"""
                Given a list of names, express the likelihood of each name being Korean using only a number between 0 and 1.
                Return the results **only as numbers in the same order as the input**, separated by spaces.

                Do not include any additional text, explanations, or formatting. Here are the names: {', '.join(remaining_names)}.
            """

            if not self.llm_api_option:
                template = PromptTemplate.from_template(template=query)
                chain = template | self.llm | StrOutputParser()
                results = chain.invoke({"names": remaining_names})
            else:
                results = self.llm_api_answer(query, model=self.llm_model)

            results = results.split()

            for index, result in enumerate(results):
                # ğŸ”¹ ìˆ«ìë§Œ ì¶”ì¶œ (ì§€ìˆ˜ í‘œê¸°ë²• ë°©ì§€)
                match = re.findall(r"\d+\.\d+|\d+", result)
                if not match:
                    value = 0.0  # ì˜ˆì™¸ ì²˜ë¦¬: ê²°ê³¼ê°€ ì—†ì„ ê²½ìš° ê¸°ë³¸ê°’
                else:
                    value = float(match[0])  # ğŸ”¹ ë¬¸ìì—´ì„ floatìœ¼ë¡œ ë³€í™˜
                    value = max(0.0, min(1.0, value))  # ğŸ”¹ ë²”ìœ„ ì¡°ì • (0.0 ~ 1.0)

                # ğŸ”¹ ì†Œìˆ˜ì  1ìë¦¬ê¹Œì§€ í¬ë§·íŒ…
                formatted_value = "{:.1f}".format(value)
                result_dict[remaining_names[index]] = formatted_value

        return {name: result_dict.get(name, "0.0") for name in names}  # ì›ë˜ ìˆœì„œ ìœ ì§€

    def llm_api_answer(self, query, model):
        # ì „ì†¡í•  ë°ì´í„°
        data = {
            "model": model,
            "prompt": query
        }

        try:
            # POST ìš”ì²­ ë³´ë‚´ê¸°
            response = requests.post(self.api_url, json=data)

            # ì‘ë‹µ í™•ì¸
            if response.status_code == 200:
                result = response.json()['response']
                result = result.replace('<think>', '').replace('</think>', '').replace('\n\n', '')
                return result
            else:
                return f"Failed to get a valid response: {response.status_code} {response.text}"

        except requests.exceptions.RequestException as e:
            return "Error communicating with the server: {e}"


    def save_name_dict(self):
        """ name_dictë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ """
        with open(self.json_filename, "w", encoding="utf-8") as file:
            json.dump(self.name_dict, file, ensure_ascii=False, indent=4) 


    def load_name_dict(self):
        """ JSON íŒŒì¼ì—ì„œ name_dict ë¶ˆëŸ¬ì˜¤ê¸° """
        if os.path.exists(self.json_filename):
            with open(self.json_filename, "r", encoding="utf-8") as file:
                return json.load(file)
        return {}  # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜


    async def authorNumChecker(self, target_author, url, session):
        try:
            stats = {
                "first_author": 0,
                "first_or_second_author": 0,
                "last_author": 0,
                "co_author": 0,
            }

            self.printStatus(f"{target_author} Paper Counting", url)
            res = await self.asyncRequester(url, session=session)
            if isinstance(res, tuple):
                # ì˜¤ë¥˜ ìƒí™© ì²˜ë¦¬: ë¡œê·¸ ê¸°ë¡ ë˜ëŠ” ê¸°ë³¸ê°’ ë°˜í™˜
                self.write_log("asyncRequester returned an error: " + str(res))
                return stats
            soup = BeautifulSoup(res, "lxml")

            publ_lists = soup.find_all('ul', class_='publ-list')
            
            trynum = 1
            while True:
                publ_lists = soup.find_all('ul', class_='publ-list')
                if publ_lists is None or len(publ_lists) == 0:
                    trynum += 1
                    if trynum == 10:
                        break
                    continue
                break
            
            papers = []
            for publ_list in publ_lists:
                publ_list = publ_list.find_all("li", class_=re.compile(r"entry"))  
                for paper in publ_list:
                    if paper.has_attr('id') and paper['id'].split('/')[1] in conf_list:
                        pass
                    else:
                        continue
                    
                    title = paper.find('span', 'title')
                    if title is not None:
                        title = title.text
                        middle = paper.find('cite', 'data tts-content')
                        authors = middle.select('span[itemprop="name"]:not(.title)')
                        author_list = [author.get_text(strip=True) for author in authors]
                        author_list.pop()

                        papers.append({
                            'title': title,
                            'authors': author_list
                        })

                for paper in papers:
                    authors = paper["authors"]
                    if target_author in authors:
                        if authors[0] == target_author:
                            stats["first_author"] += 1
                            stats["first_or_second_author"] += 1  # 1ì €ìë„ 2ì €ì ì¡°ê±´ì— í¬í•¨ë¨
                        elif len(authors) > 1 and authors[1] == target_author:
                            stats["first_or_second_author"] += 1
                        elif authors[-1] == target_author:
                            stats["last_author"] += 1
                        stats["co_author"] += 1

            return f"({stats['first_author']},{stats['first_or_second_author']},{stats['last_author']},{stats['co_author']})"
        except Exception as e:
            self.write_log(traceback.format_exc())
            return stats


    def printStatus(self, msg='', url=None):
        try:
            print(f'\r{msg} | {url} | paper: {len(self.CrawlData)} | Korean Authors: {len(self.checkedNameList)}', end='')
        except:
            pass


    def random_heador(self):
        navigator = generate_navigator()
        navigator = navigator['user_agent']
        return {"User-Agent": navigator}


    def random_proxy(self):
        proxy_server = random.choice(self.proxy_list)
        if self.proxy_option == True:
            return {"http": 'http://' + proxy_server, 'https': 'http://' + proxy_server}
        else:
            return None


    def write_log(self, message):
        # í˜„ì¬ ì‹œê°„ ì¶”ê°€
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_message = f"[{timestamp}] {message}\n"

        # ë¡œê·¸ íŒŒì¼ì— ë©”ì‹œì§€ ì¶”ê°€
        with open(self.log_file_path, 'a') as file:
            file.write(log_message)


    def Requester(self, url, headers={}, params={}, proxies={}, cookies={}):
        try:
            if headers == {}:
                headers = self.random_heador()

            if self.proxy_option == True:
                trynum = 0
                while True:
                    proxies = self.random_proxy()
                    try:
                        main_page = requests.get(url, proxies=proxies, headers=headers, params=params,
                                                 cookies=cookies, verify=False, timeout=TIMEOUT)
                        return main_page
                    except Exception as e:
                        if trynum >= TRYNUM:
                            return ("ERROR", traceback.format_exc())
                        trynum += 1
            else:
                return requests.get(url, headers=headers, params=params, verify=False)

        except Exception as e:
             return ("ERROR", traceback.format_exc())
    

    async def asyncRequester(self, url, headers={}, params={}, proxies='', cookies={}, session=None):
        timeout = aiohttp.ClientTimeout(total=TIMEOUT)
        trynum = 0
        while True:
            try:
                if self.proxy_option:
                    proxies = self.async_proxy()
                headers = self.random_heador()
                async with session.get(url, headers=headers, params=params, proxy=proxies, cookies=cookies,
                                       ssl=False, timeout=TIMEOUT) as response:
                    return await response.text()
            except (aiohttp.ClientError, asyncio.TimeoutError, Exception) as e:
                if trynum >= TRYNUM:
                    return self.error_dump(1003, self.error_detector(), url)
                trynum += 1


    def clear_console(self):
        if platform.system() == "Windows":
            os.system("cls")
        else:
            os.system("clear")

if __name__ == "__main__":
    pcssearch_obj = PCSSEARCH(1, 0.5, 2024, 2024)
    conf_list = ['CCS']
    pcssearch_obj.main(conf_list)
    
    
